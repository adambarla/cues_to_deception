q95_z <- get_q(0.95)
q99_z <- get_q(0.99)
} else {
# trop peu d'excès ⇒ on utilise pur empirie
q95_z <- quantile(z_win, 0.95, type = 1)
q99_z <- quantile(z_win, 0.99, type = 1)
}
# g) VaR pour t+1
VaR95[j] <- mu1 + sigma1 * q95_z
VaR99[j] <- mu1 + sigma1 * q99_z
}
# 4. Assemblage final
VaR_pred_df <- data.frame(
Date   = ret_df$Date[test_idx],
mu     = mu_pred,
sigma  = sigma_pred,
VaR95  = VaR95,
VaR99  = VaR99
)
# 5. Affichage
print(head(VaR_pred_df, 10))
n_test
#| label: "BackTest1"
#| echo: TRUE
#| message: FALSE
#| warning: FALSE
#| error: FALSE
#| fig.height: 4
#| fig.width:  6
## 1. Fusion prévisions / pertes réalisées ---------------------------
fc_tbl <- VaR_pred_df |>
left_join(
ret_df |> select(Date, Loss = portf_loss),
by = "Date"
)
## 2. Comptage des violations ----------------------------------------
viol_95 <- fc_tbl$Loss > fc_tbl$VaR95
viol_99 <- fc_tbl$Loss > fc_tbl$VaR99
cat("Violations 95% :", sum(viol_95), "/", nrow(fc_tbl), "\n")
cat("Violations 99% :", sum(viol_99), "/", nrow(fc_tbl), "\n")
## 3. Tests de couverture (Kupiec) -----------------------------------
test_95 <- VaRTest(alpha = 0.95,
actual = -fc_tbl$Loss,
VaR    = -fc_tbl$VaR95)
test_99 <- VaRTest(alpha = 0.99,
actual = -fc_tbl$Loss,
VaR    = -fc_tbl$VaR99)
print(test_95)
print(test_99)
## 4. Visualisation ---------------------------------------------------
p_loss_var <- ggplot(fc_tbl, aes(Date)) +
geom_line(aes(y = Loss),  colour = "black", size = .4) +
geom_line(aes(y = VaR95), colour = "red",   size = .4) +
geom_line(aes(y = VaR99), colour = "blue",  size = .4) +
theme_bw() +
labs(title    = "Portfolio Losses vs Predicted VaR (rolling 100 days)",
y        = "Value",
x        = NULL,
caption  = "Black = realised loss | Red = VaR 95 % | Blue = VaR 99 %") +
theme(plot.title = element_text(hjust = .5, face = "bold"),
axis.text  = element_text(colour = "black"),
axis.title = element_text(face = "bold"))
print(p_loss_var)
# (optionnel) enregistrer la figure
ggsave("Loss_vs_VaR.pdf", p_loss_var, width = 7, height = 4)
#| label: "BackTest2"
#| echo: TRUE
#| message: FALSE
#| warning: FALSE
#| error: FALSE
#| fig.height: 4
#| fig.width:  6
# ──────────────────────────────────────────────────────────────────────────────
# Step A.7 : binomial back‑testing des violations de VaR
# ──────────────────────────────────────────────────────────────────────────────
alpha_vec <- c(0.95, 0.99)      # niveaux de confiance à tester
results   <- list()             # pour stocker les sorties
for (a in alpha_vec) {
VaR_col <- ifelse(a == 0.95, "VaR95", "VaR99")
# 1. Dépassements (Loss > VaR) sur l’horizon de test
violations <- fc_tbl$Loss > fc_tbl[[VaR_col]]
x          <- sum(violations)         # nombre de violations
T          <- nrow(fc_tbl)            # taille de l’échantillon
p0         <- 1 - a                   # probabilité théorique de violation
# 2. Test binomial bilatéral
bin_test <- binom.test(x, T, p = p0, alternative = "two.sided")
# 3. Affichage concis
cat("\n──────────  VaR", a*100, "%  ──────────\n")
cat("Violations observées   : ", x, "/", T,
" (", round(x/T*100, 3), "%)\n", sep = "")
cat("Violations attendues    : ", round(T * p0, 1),
" (", p0*100, "%)\n", sep = "")
cat("p‑value binomiale       : ", format(bin_test$p.value, digits = 4), "\n", sep = "")
cat("IC 95 % sur le taux obs : [",
paste(round(bin_test$conf.int, 4), collapse = "; "), "]\n", sep = "")
# 4. Stockage
results[[paste0("VaR", a*100)]] <- list(
x      = x,
T      = T,
p0     = p0,
test   = bin_test
)
}
###############################################################################
#                    RISK QUANTIFICATION IN AN EQUITY PORTFOLIO               #
#               EPFL ‒ Applied Statistics Project (2024/25)                   #
#                              Author: Kalil Bouhadra                         #
###############################################################################
#=============================================================================
# SECTION 1 : INITIALISATION
#=============================================================================
set.seed(12345)
library(ggplot2)
library(dplyr)
library(moments)
library(rugarch)
library(POT)
library(purrr)
library(PerformanceAnalytics)
#=============================================================================
# SECTION 2 : LECTURE DES DONNÉES ET MISE EN FORME
#=============================================================================
load("dataset_05.Rdata")
library(readr)
library(ggplot2)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data <- read_csv("processed_data.csv")
#### descriptive analysis ####
## summaries
summary(data$group1_score) # strongly aligns with literature
summary(data$group2_score[data$cue_group=="GoodCues"]) # identical
summary(data$group2_score[data$cue_group=="BadCues"]) # cool
##
table(data$cue_group)
## barplot next to one another
# data for the plot
freq_g1 <- as.vector(prop.table(table(data$group1_score)))
freq_bad <- as.vector(prop.table(table(data$group2_score[data$cue_group=="BadCues"])))
freq_good <- as.vector(prop.table(table(data$group2_score[data$cue_group=="GoodCues"])))
data_long <- data.frame(
category = round(rep(seq(0,1, length.out = 4),3),3),
Legend =  c(rep("No cues", 4), rep("bad cues", 4), rep("good cues", 4)),
value = c(freq_g1, freq_bad, freq_good)
)
# plot
ggplot(data_long, aes(x = factor(category), y = value, fill = Legend)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Distributions of the scores",
x = "Success rate",
y = "Frequency") +
theme_minimal()
## improvements
data$difference <- (data$group2_score-data$group1_score)*3
table(data$difference, data$cue_group)
data$cue_group <- factor(data$cue_group,
levels = c("BadCues", "GoodCues"),
labels = c("bad cues", "good cues"))
ggplot(data, aes(x = factor(difference), fill = cue_group)) +
geom_bar( alpha = 1, position = "dodge") +
scale_fill_manual(values = c("bad cues" = "#F8766D", "good cues" = "#00BA38")) +
labs(
title = "Distributions of Score Differences",
x = "Difference",
y = "Count",
fill = "Legend"
) +
theme_minimal()
### random bullshit
big_data <- read_csv("processed_data_all_columns.csv")
## nationality
tapply(big_data$group1_score, big_data$nationality, mean)
table(big_data$nationality)
tapply(big_data$group2_score, big_data$nationality, mean)[c("Italy", "Morocco", "Lebanon", "France")]
tapply(big_data$group1_score, big_data$nationality, mean)[c("Italy", "Morocco", "Lebanon", "France")]
means_n1 <- tapply(big_data$group1_score, big_data$nationality, mean)[c("Italy", "Morocco", "Lebanon", "France")]
means_n2 <- tapply(big_data$group2_score, big_data$nationality, mean)[c("Italy", "Morocco", "Lebanon", "France")]
library(tibble)
library(tidyr)
df_nations <- tibble(
nationality = rep(c("Italy", "Morocco", "Lebanon", "France"), 2),
score_group = rep(c("Group 1", "Group 2"), each = 4),
mean_score = c(means_n1, means_n2)
)
ggplot(df_nations, aes(x = score_group, y = mean_score, fill = nationality)) +
geom_bar(stat = "identity", position = "dodge", color = "white", alpha = 0.9) +
scale_fill_manual(values = c("France" = "#0055A4", "Italy" = "#007FFF", "Lebanon" = "#00BA38", "Morocco"= "firebrick")) +
labs(
title = "Average Scores by Nationality",
x = "Nationality",
y = "Mean Score",
fill = "Legend"
) +
theme_minimal()
## gender
means_s1 <- tapply(big_data$group1_score, big_data$gender, mean)[1:2]
means_s2 <- tapply(big_data$group2_score, big_data$gender, mean)[1:2]
df_genders <- tibble(
gender = rep(c("Female", "Male"), 2),
score_group = rep(c("Group 1", "Group 2"), each = 2),
mean_score = c(means_s1, means_s2)
)
ggplot(df_genders, aes(x = score_group, y = mean_score, fill = gender)) +
geom_bar(stat = "identity", position = "dodge", color = "white", alpha = 0.9) +
scale_fill_manual(values = c("Female" = "pink", "Male" = "#0055A4")) +
labs(
title = "Average Scores by Gender",
x = "gender",
y = "Mean Score",
fill = "Legend"
) +
theme_minimal()
##################
#### testing ####
## bad cues
bad_cues_df <- data.frame(
y = c(data$group1_score[data$cue_group=="bad cues"], data$group2_score[data$cue_group=="bad cues"]),
group = c(rep("Before cues", sum(data$cue_group=="bad cues")), rep("After cues", sum(data$cue_group=="bad cues")))
)
t.test(y ~ group, data = bad_cues_df)
## good cues
good_cues_df <- data.frame(
y = c(data$group1_score[data$cue_group=="good cues"], data$group2_score[data$cue_group=="good cues"]),
group = c(rep("Before cues", sum(data$cue_group=="good cues")), rep("After cues", sum(data$cue_group=="good cues")))
)
t.test(y ~ group, data = good_cues_df)
### --- TEST 1 ---
# Do participants given GOOD cues show significantly greater improvement
# in lie detection performance than those given BAD cues?
# We test whether the average score change (difference) is higher for the "good cues" group.
t.test(
difference ~ cue_group,
data = data,
alternative = "greater"  # one-sided test because we expect good cues > bad cues
)
### --- TEST 2 ---
# Does giving any kind of cues (good or bad) influence performance overall?
# This ignores the distinction between good and bad cues.
# We compare pre- and post-cue performance for all participants combined.
t.test(
x = data$group1_score,     # performance before cues
y = data$group2_score,     # performance after cues
paired = TRUE,             # same participants tested before and after
alternative = "two.sided"  # we test for any change (increase or decrease)
)
### random bullshit
big_data <- read_csv("processed_data_all_columns.csv")
## Écart-type de l'âge
sd(big_data$age, na.rm = TRUE)
## Âge minimum et maximum
range(big_data$age, na.rm = TRUE)
library(readr)
library(ggplot2)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data <- read_csv("processed_data.csv")
#### descriptive analysis ####
## summaries
summary(data$group1_score) # strongly aligns with literature
summary(data$group2_score[data$cue_group=="GoodCues"]) # identical
summary(data$group2_score[data$cue_group=="BadCues"]) # cool
##
table(data$cue_group)
## barplot next to one another
# data for the plot
freq_g1 <- as.vector(prop.table(table(data$group1_score)))
freq_bad <- as.vector(prop.table(table(data$group2_score[data$cue_group=="BadCues"])))
freq_good <- as.vector(prop.table(table(data$group2_score[data$cue_group=="GoodCues"])))
data_long <- data.frame(
category = round(rep(seq(0,1, length.out = 4),3),3),
Legend =  c(rep("No cues", 4), rep("bad cues", 4), rep("good cues", 4)),
value = c(freq_g1, freq_bad, freq_good)
)
# plot
ggplot(data_long, aes(x = factor(category), y = value, fill = Legend)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Distributions of the scores",
x = "Success rate",
y = "Frequency") +
theme_minimal()
## improvements
data$difference <- (data$group2_score-data$group1_score)*3
table(data$difference, data$cue_group)
data$cue_group <- factor(data$cue_group,
levels = c("BadCues", "GoodCues"),
labels = c("bad cues", "good cues"))
ggplot(data, aes(x = factor(difference), fill = cue_group)) +
geom_bar( alpha = 1, position = "dodge") +
scale_fill_manual(values = c("bad cues" = "#F8766D", "good cues" = "#00BA38")) +
labs(
title = "Distributions of Score Differences",
x = "Difference",
y = "Count",
fill = "Legend"
) +
theme_minimal()
### random bullshit
big_data <- read_csv("processed_data_all_columns.csv")
## Écart-type de l'âge
sd(big_data$age, na.rm = TRUE)
## Âge minimum et maximum
range(big_data$age, na.rm = TRUE)
## nationality
tapply(big_data$group1_score, big_data$nationality, mean)
table(big_data$nationality)
tapply(big_data$group2_score, big_data$nationality, mean)[c("Italy", "Morocco", "Lebanon", "France")]
tapply(big_data$group1_score, big_data$nationality, mean)[c("Italy", "Morocco", "Lebanon", "France")]
means_n1 <- tapply(big_data$group1_score, big_data$nationality, mean)[c("Italy", "Morocco", "Lebanon", "France")]
means_n2 <- tapply(big_data$group2_score, big_data$nationality, mean)[c("Italy", "Morocco", "Lebanon", "France")]
library(tibble)
library(tidyr)
df_nations <- tibble(
nationality = rep(c("Italy", "Morocco", "Lebanon", "France"), 2),
score_group = rep(c("Group 1", "Group 2"), each = 4),
mean_score = c(means_n1, means_n2)
)
ggplot(df_nations, aes(x = score_group, y = mean_score, fill = nationality)) +
geom_bar(stat = "identity", position = "dodge", color = "white", alpha = 0.9) +
scale_fill_manual(values = c("France" = "#0055A4", "Italy" = "#007FFF", "Lebanon" = "#00BA38", "Morocco"= "firebrick")) +
labs(
title = "Average Scores by Nationality",
x = "Nationality",
y = "Mean Score",
fill = "Legend"
) +
theme_minimal()
## gender
means_s1 <- tapply(big_data$group1_score, big_data$gender, mean)[1:2]
means_s2 <- tapply(big_data$group2_score, big_data$gender, mean)[1:2]
df_genders <- tibble(
gender = rep(c("Female", "Male"), 2),
score_group = rep(c("Group 1", "Group 2"), each = 2),
mean_score = c(means_s1, means_s2)
)
ggplot(df_genders, aes(x = score_group, y = mean_score, fill = gender)) +
geom_bar(stat = "identity", position = "dodge", color = "white", alpha = 0.9) +
scale_fill_manual(values = c("Female" = "pink", "Male" = "#0055A4")) +
labs(
title = "Average Scores by Gender",
x = "gender",
y = "Mean Score",
fill = "Legend"
) +
theme_minimal()
##################
#### testing ####
## bad cues
bad_cues_df <- data.frame(
y = c(data$group1_score[data$cue_group=="bad cues"], data$group2_score[data$cue_group=="bad cues"]),
group = c(rep("Before cues", sum(data$cue_group=="bad cues")), rep("After cues", sum(data$cue_group=="bad cues")))
)
t.test(y ~ group, data = bad_cues_df)
## good cues
good_cues_df <- data.frame(
y = c(data$group1_score[data$cue_group=="good cues"], data$group2_score[data$cue_group=="good cues"]),
group = c(rep("Before cues", sum(data$cue_group=="good cues")), rep("After cues", sum(data$cue_group=="good cues")))
)
t.test(y ~ group, data = good_cues_df)
### --- TEST 1 ---
# Do participants given GOOD cues show significantly greater improvement
# in lie detection performance than those given BAD cues?
# We test whether the average score change (difference) is higher for the "good cues" group.
t.test(
difference ~ cue_group,
data = data,
alternative = "greater"  # one-sided test because we expect good cues > bad cues
)
### --- TEST 2 ---
# Does giving any kind of cues (good or bad) influence performance overall?
# This ignores the distinction between good and bad cues.
# We compare pre- and post-cue performance for all participants combined.
t.test(
x = data$group1_score,     # performance before cues
y = data$group2_score,     # performance after cues
paired = TRUE,             # same participants tested before and after
alternative = "two.sided"  # we test for any change (increase or decrease)
)
### random bullshit
big_data <- read_csv("processed_data_all_columns.csv")
## Écart-type de l'âge
sd(big_data$age_1, na.rm = TRUE)
## Âge minimum et maximum
range(big_data$age_1, na.rm = TRUE)
library(readr)
library(ggplot2)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data <- read_csv("processed_data.csv")
#### descriptive analysis ####
## summaries
summary(data$group1_score) # strongly aligns with literature
summary(data$group2_score[data$cue_group=="GoodCues"]) # identical
summary(data$group2_score[data$cue_group=="BadCues"]) # cool
##
table(data$cue_group)
## barplot next to one another
# data for the plot
freq_g1 <- as.vector(prop.table(table(data$group1_score)))
freq_bad <- as.vector(prop.table(table(data$group2_score[data$cue_group=="BadCues"])))
freq_good <- as.vector(prop.table(table(data$group2_score[data$cue_group=="GoodCues"])))
data_long <- data.frame(
category = round(rep(seq(0,1, length.out = 4),3),3),
Legend =  c(rep("No cues", 4), rep("bad cues", 4), rep("good cues", 4)),
value = c(freq_g1, freq_bad, freq_good)
)
# plot
ggplot(data_long, aes(x = factor(category), y = value, fill = Legend)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Distributions of the scores",
x = "Success rate",
y = "Frequency") +
theme_minimal()
## improvements
data$difference <- (data$group2_score-data$group1_score)*3
table(data$difference, data$cue_group)
data$cue_group <- factor(data$cue_group,
levels = c("BadCues", "GoodCues"),
labels = c("bad cues", "good cues"))
ggplot(data, aes(x = factor(difference), fill = cue_group)) +
geom_bar( alpha = 1, position = "dodge") +
scale_fill_manual(values = c("bad cues" = "#F8766D", "good cues" = "#00BA38")) +
labs(
title = "Distributions of Score Differences",
x = "Difference",
y = "Count",
fill = "Legend"
) +
theme_minimal()
### random bullshit
big_data <- read_csv("processed_data_all_columns.csv")
## Écart-type de l'âge
sd(big_data$age_1, na.rm = TRUE)
## Âge minimum et maximum
range(big_data$age_1, na.rm = TRUE)
## nationality
tapply(big_data$group1_score, big_data$nationality, mean)
table(big_data$nationality)
tapply(big_data$group2_score, big_data$nationality, mean)[c("Italy", "Morocco", "Lebanon", "France")]
tapply(big_data$group1_score, big_data$nationality, mean)[c("Italy", "Morocco", "Lebanon", "France")]
means_n1 <- tapply(big_data$group1_score, big_data$nationality, mean)[c("Italy", "Morocco", "Lebanon", "France")]
means_n2 <- tapply(big_data$group2_score, big_data$nationality, mean)[c("Italy", "Morocco", "Lebanon", "France")]
library(tibble)
library(tidyr)
df_nations <- tibble(
nationality = rep(c("Italy", "Morocco", "Lebanon", "France"), 2),
score_group = rep(c("Group 1", "Group 2"), each = 4),
mean_score = c(means_n1, means_n2)
)
ggplot(df_nations, aes(x = score_group, y = mean_score, fill = nationality)) +
geom_bar(stat = "identity", position = "dodge", color = "white", alpha = 0.9) +
scale_fill_manual(values = c("France" = "#0055A4", "Italy" = "#007FFF", "Lebanon" = "#00BA38", "Morocco"= "firebrick")) +
labs(
title = "Average Scores by Nationality",
x = "Nationality",
y = "Mean Score",
fill = "Legend"
) +
theme_minimal()
## gender
means_s1 <- tapply(big_data$group1_score, big_data$gender, mean)[1:2]
means_s2 <- tapply(big_data$group2_score, big_data$gender, mean)[1:2]
df_genders <- tibble(
gender = rep(c("Female", "Male"), 2),
score_group = rep(c("Group 1", "Group 2"), each = 2),
mean_score = c(means_s1, means_s2)
)
ggplot(df_genders, aes(x = score_group, y = mean_score, fill = gender)) +
geom_bar(stat = "identity", position = "dodge", color = "white", alpha = 0.9) +
scale_fill_manual(values = c("Female" = "pink", "Male" = "#0055A4")) +
labs(
title = "Average Scores by Gender",
x = "gender",
y = "Mean Score",
fill = "Legend"
) +
theme_minimal()
##################
#### testing ####
## bad cues
bad_cues_df <- data.frame(
y = c(data$group1_score[data$cue_group=="bad cues"], data$group2_score[data$cue_group=="bad cues"]),
group = c(rep("Before cues", sum(data$cue_group=="bad cues")), rep("After cues", sum(data$cue_group=="bad cues")))
)
t.test(y ~ group, data = bad_cues_df)
## good cues
good_cues_df <- data.frame(
y = c(data$group1_score[data$cue_group=="good cues"], data$group2_score[data$cue_group=="good cues"]),
group = c(rep("Before cues", sum(data$cue_group=="good cues")), rep("After cues", sum(data$cue_group=="good cues")))
)
t.test(y ~ group, data = good_cues_df)
### --- TEST 1 ---
# Do participants given GOOD cues show significantly greater improvement
# in lie detection performance than those given BAD cues?
# We test whether the average score change (difference) is higher for the "good cues" group.
t.test(
difference ~ cue_group,
data = data,
alternative = "greater"  # one-sided test because we expect good cues > bad cues
)
### --- TEST 2 ---
# Does giving any kind of cues (good or bad) influence performance overall?
# This ignores the distinction between good and bad cues.
# We compare pre- and post-cue performance for all participants combined.
t.test(
x = data$group1_score,     # performance before cues
y = data$group2_score,     # performance after cues
paired = TRUE,             # same participants tested before and after
alternative = "two.sided"  # we test for any change (increase or decrease)
)
library(readr)
library(ggplot2)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(readr)
library(ggplot2)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
